{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "FzOuiQoTObOB",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-18T19:15:52.976080Z",
     "end_time": "2025-02-18T19:15:52.997105Z"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1739893809115,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     },
     "user_tz": -120
    },
    "id": "FzOuiQoTObOB"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "LOG_DIR = \"./Logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def remove_logger_handlers(logger_name):\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "class CoolLogger:\n",
    "    def __init__(self, name=\"CoolLogger\", log_file=\"app.log\", level=logging.DEBUG):\n",
    "        \"\"\"\n",
    "        Initializes the logger with console and file handlers.\n",
    "\n",
    "        :param name: Logger name\n",
    "        :param log_file: Log file path\n",
    "        :param level: Logging level (default: DEBUG)\n",
    "        \"\"\"\n",
    "        remove_logger_handlers(name)\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(level)\n",
    "\n",
    "        log_format = logging.Formatter(\n",
    "            \"%(asctime)s | %(levelname)s | %(message)s\", \"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(self._get_colored_formatter())\n",
    "\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(log_format)\n",
    "\n",
    "        self.logger.addHandler(console_handler)\n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "    def _get_colored_formatter(self):\n",
    "        \"\"\" Returns a colorized log formatter \"\"\"\n",
    "        class ColoredFormatter(logging.Formatter):\n",
    "            COLORS = {\n",
    "                \"DEBUG\": \"\\033[94m\",  # Blue\n",
    "                \"INFO\": \"\\033[92m\",  # Green\n",
    "                \"WARNING\": \"\\033[93m\",  # Yellow\n",
    "                \"ERROR\": \"\\033[91m\",  # Red\n",
    "                \"CRITICAL\": \"\\033[41m\",  # Background Red\n",
    "                \"RESET\": \"\\033[0m\",\n",
    "            }\n",
    "\n",
    "            def format(self, record):\n",
    "                log_color = self.COLORS.get(record.levelname, self.COLORS[\"RESET\"])\n",
    "                reset = self.COLORS[\"RESET\"]\n",
    "                return f\"{log_color}{record.levelname} | {record.getMessage()}{reset}\"\n",
    "\n",
    "        return ColoredFormatter(\"%(levelname)s | %(message)s\")\n",
    "\n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def warning(self, message):\n",
    "        self.logger.warning(message)\n",
    "\n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "\n",
    "    def critical(self, message):\n",
    "        self.logger.critical(message)\n",
    "\n",
    "\n",
    "logging.root.handlers.clear()\n",
    "remove_logger_handlers(\"RepoHandlerLogger\")\n",
    "remove_logger_handlers(\"ModelLogger\")\n",
    "\n",
    "# Initialize loggers\n",
    "log = CoolLogger(name=\"RepoHandlerLogger\", log_file=f\"{LOG_DIR}/repo_handler.log\")\n",
    "model_log = CoolLogger(name=\"ModelLogger\", log_file=f\"{LOG_DIR}/xg_trainer.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "X69rUbz6OYNX",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-18T19:15:57.925057Z",
     "end_time": "2025-02-18T19:15:57.959056Z"
    },
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1739893812815,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     },
     "user_tz": -120
    },
    "id": "X69rUbz6OYNX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class repo_handler:\n",
    "\n",
    "    def __init__(self):\n",
    "        log.info(\"Initializing Repository Handler\")\n",
    "        self.malicious_data = []\n",
    "        self.malicious_data_df = []\n",
    "        self.LFI_df = []\n",
    "        self.SQLi_df = []\n",
    "        self.XSS_df = []\n",
    "        self.RFI_df = []\n",
    "        self.RCE_df = []\n",
    "        self.normal_data = []\n",
    "        self.normal_data_df = []\n",
    "        self.excel_file = \"./Repository/Dataset.xlsx\"\n",
    "\n",
    "        try:\n",
    "            self.load_data()\n",
    "            self.full_data = pd.concat([self.normal_data, self.malicious_data], ignore_index=True)\n",
    "            self.malicious_data_df = pd.concat([self.RCE_df,self.LFI_df,self.RFI_df,self.SQLi_df,self.XSS_df])\n",
    "            self.normal_data_df = pd.DataFrame(self.normal_data, columns=[\"UriQuery\"])\n",
    "            self.normal_data_df[\"category\"] = \"Benign\"\n",
    "            self.malicious_data_df[\"isVulnerable\"] = 1 # 1 for malicious\n",
    "            self.normal_data_df[\"isVulnerable\"] = 0 # 0 for normal\n",
    "            self.full_data_df = pd.concat([self.malicious_data_df,self.normal_data_df],ignore_index=True)\n",
    "            self.randomize_n_time(5)\n",
    "            log.info(\"Repository successfully initialized!\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during initialization: {e}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\" Loads and processes data from the Excel file. \"\"\"\n",
    "        log.info(\"Loading Data from Excel...\")\n",
    "\n",
    "        try:\n",
    "            excel_data = pd.read_excel(self.excel_file, sheet_name=None)\n",
    "            data = {\n",
    "                sheet_name: d[\"UriQuery\"].drop_duplicates().dropna()\n",
    "                for sheet_name, d in excel_data.items() if \"UriQuery\" in d.columns\n",
    "            }\n",
    "\n",
    "            if not data:\n",
    "                log.warning(\"No data found in the Excel file!\")\n",
    "\n",
    "            self.normal_data = data[\"Benign\"]\n",
    "            self.RCE_df = pd.DataFrame(data[\"RCE\"], columns=[\"UriQuery\"])\n",
    "            self.RCE_df[\"category\"] = \"RCE\"\n",
    "\n",
    "            self.LFI_df = pd.DataFrame(data[\"LFI\"], columns=[\"UriQuery\"])\n",
    "            self.LFI_df[\"category\"] = \"LFI\"\n",
    "\n",
    "            self.SQLi_df = pd.DataFrame(data[\"SQLi\"], columns=[\"UriQuery\"])\n",
    "            self.SQLi_df[\"category\"] = \"SQLi\"\n",
    "\n",
    "            self.XSS_df = pd.DataFrame(data[\"XSS\"], columns=[\"UriQuery\"])\n",
    "            self.XSS_df[\"category\"] = \"XSS\"\n",
    "\n",
    "            self.RFI_df = pd.DataFrame(data[\"RFI\"], columns=[\"UriQuery\"])\n",
    "            self.RFI_df[\"category\"] = \"RFI\"\n",
    "\n",
    "\n",
    "            self.malicious_data = pd.concat([data[\"RCE\"],data[\"LFI\"],data[\"SQLi\"],data[\"XSS\"],data[\"RFI\"]], ignore_index=True)\n",
    "            log.info(f\"Loaded {len(data['SQLi'])} SQLi, {len(data['RFI'])} RFI, {len(data['LFI'])} LFI, {len(data['XSS'])} XSS and {len(data['RCE'])} RCE entries.\")\n",
    "            log.info(f\"Loaded {len(self.normal_data)} normal and {len(self.malicious_data)} malicious entries.\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error loading data: {e}\")\n",
    "\n",
    "    def randomize_n_time(self, counter):\n",
    "        \"\"\" Randomizes the dataset multiple times. \"\"\"\n",
    "        log.info(f\"Randomizing dataset {counter} times...\")\n",
    "\n",
    "        try:\n",
    "            for _ in range(counter):\n",
    "                self.full_data_df = self.full_data_df.sample(frac=1).reset_index(drop=True)\n",
    "            log.info(\"Dataset successfully randomized!\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during randomization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lVh50_RPg3ug",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-18T19:16:04.510547Z",
     "end_time": "2025-02-18T19:16:10.913939Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7136,
     "status": "ok",
     "timestamp": 1739893822161,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     },
     "user_tz": -120
    },
    "id": "lVh50_RPg3ug",
    "outputId": "344cf909-cade-43dd-9c23-7ea1e7500351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "WHZBoQepk9Es",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T17:59:10.893747Z",
     "start_time": "2025-02-18T17:59:10.818555Z"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1739893822161,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     },
     "user_tz": -120
    },
    "id": "WHZBoQepk9Es"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class WebAttackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "          item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "in8Vy9nqOQOw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T17:59:15.941128Z",
     "start_time": "2025-02-18T17:59:10.836813Z"
    },
    "executionInfo": {
     "elapsed": 22320,
     "status": "ok",
     "timestamp": 1739893844443,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     },
     "user_tz": -120
    },
    "id": "in8Vy9nqOQOw"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_imaging' from 'PIL' (d:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\PIL\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#from datasets import load_dataset\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
      "File \u001B[1;32md:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\transformers\\__init__.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     OptionalDependencyNotAvailable,\n\u001B[0;32m     29\u001B[0m     _LazyModule,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m     logging,\n\u001B[0;32m     49\u001B[0m )\n\u001B[0;32m     52\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mget_logger(\u001B[38;5;18m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "File \u001B[1;32md:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdependency_versions_table\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[0;32m     25\u001B[0m pkgs_to_check_at_runtime \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtqdm\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyyaml\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     38\u001B[0m ]\n",
      "File \u001B[1;32md:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\transformers\\utils\\__init__.py:27\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackbone_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BackboneConfigMixin, BackboneMixin\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat_template_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdoc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     30\u001B[0m     add_code_sample_docstrings,\n\u001B[0;32m     31\u001B[0m     add_end_docstrings,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     35\u001B[0m     replace_return_docstrings,\n\u001B[0;32m     36\u001B[0m )\n",
      "File \u001B[1;32md:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py:37\u001B[0m\n\u001B[0;32m     34\u001B[0m     jinja2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_vision_available():\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mImage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor\n",
      "File \u001B[1;32md:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\PIL\\Image.py:84\u001B[0m\n\u001B[0;32m     75\u001B[0m MAX_IMAGE_PIXELS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m1024\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1024\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1024\u001B[39m \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m4\u001B[39m \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001B[39;00m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001B[39;00m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;66;03m# import Image and use the Image.core variable instead.\u001B[39;00m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001B[39;00m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;66;03m# and should be considered private and subject to change.\u001B[39;00m\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _imaging \u001B[38;5;28;01mas\u001B[39;00m core\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m __version__ \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(core, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPILLOW_VERSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     87\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     89\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCore version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mgetattr\u001B[39m(core,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPILLOW_VERSION\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     90\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPillow version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     91\u001B[0m         )\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name '_imaging' from 'PIL' (d:\\Setups\\Conda\\envs\\AIM\\lib\\site-packages\\PIL\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
    "#from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self,data_frame=None):\n",
    "        self.data = data_frame\n",
    "        self.model = None\n",
    "        self.id2label,self.label2id = self.labelization()\n",
    "        #model_log.info(f\"id2label: {self.id2label}\")\n",
    "        model_log.info(f\"label2id: {self.label2id}\")\n",
    "        self.represent()\n",
    "        self.train_model()\n",
    "        #self.create_model()\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "      X_train, X_val, y_train, y_val = train_test_split(list(self.data[\"UriQuery\"]),list(self.data[\"categories_label\"]), test_size=0.2,stratify=self.data[\"categories_label\"])\n",
    "      tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "      X_train_tokenized = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "      X_val_tokineized = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "      train_dataset = WebAttackDataset(X_train_tokenized, y_train)\n",
    "      val_dataset = WebAttackDataset(X_val_tokineized, y_val)\n",
    "      training_args = TrainingArguments(\n",
    "          output_dir=\"./Results\",\n",
    "          num_train_epochs=3,\n",
    "          per_device_train_batch_size=16\n",
    "      )\n",
    "\n",
    "      self.load_model()\n",
    "\n",
    "      trainer = Trainer(\n",
    "          model=self.model,\n",
    "          args=training_args,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=val_dataset,\n",
    "          compute_metrics=self.compute_metric\n",
    "\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "      trainer.evaluate()\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "      #print(self.model)\n",
    "\n",
    "      training_args = TrainingArguments(\n",
    "      output_dir=\"./Results\",  # Directory to store model and logs\n",
    "      num_train_epochs=3,      # Number of epochs\n",
    "      per_device_train_batch_size=16,  # Batch size for training\n",
    "      per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "      eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "      save_strategy=\"epoch\",          # Save model at the end of each epoch\n",
    "      logging_dir=\"./Logs\",           # Directory for logs\n",
    "      logging_steps=200,              # Log after every 200 steps\n",
    "      load_best_model_at_end=True,    # Load the best model based on evaluation\n",
    "      metric_for_best_model=\"accuracy\",  # Metric to track for the best model\n",
    "      # Optional: To improve checkpoint saving\n",
    "      save_total_limit=3,             # Keep only the last 3 saved checkpoints\n",
    "      weight_decay=0.01,              # Apply weight decay to reduce overfitting\n",
    "      warmup_steps=500,               # Warm-up for the first 500 steps to stabilize training\n",
    "      fp16=True,                      # Use mixed precision (if supported by your hardware for faster training)\n",
    "      seed=42,                        # Set random seed for reproducibility\n",
    "      )\n",
    "\n",
    "      trainer = Trainer(\n",
    "      model=self.model,                  # Pretrained BERT model\n",
    "      args=training_args,           # Training arguments\n",
    "      train_dataset=train_dataset,  # Training dataset\n",
    "      eval_dataset=val_dataset,     # Validation dataset\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "\n",
    "      results = trainer.evaluate()\n",
    "      print(results)\n",
    "      \"\"\"\n",
    "\n",
    "    def compute_metric(self,p):\n",
    "      print(type(p))\n",
    "      pred, labels = p\n",
    "      pred = np.argmax(pred, axis=1)\n",
    "\n",
    "      accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "      recall = recall_score(y_true=labels, y_pred=pred)\n",
    "      precision = precision_score(y_true=labels, y_pred=pred)\n",
    "      f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "      return {\"Accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    def labelization(self):\n",
    "      labels = self.data['category'].unique().tolist()\n",
    "      labels = [s.strip() for s in labels]\n",
    "      return {id: label for id,label in enumerate(labels)},{label: id for id,label in enumerate(labels)}\n",
    "\n",
    "    def represent(self):\n",
    "      self.data[\"categories_label\"] = pd.factorize(self.data.category)[0]\n",
    "\n",
    "    def load_model(self):\n",
    "      self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(self.label2id))\n",
    "\n",
    "\n",
    "    def viewGraph(self):\n",
    "      self.data['category'].value_counts().plot(\n",
    "      kind='pie',\n",
    "      figsize=(8, 8),\n",
    "      autopct=lambda p: f'{p:.2f}%'\n",
    "      )\n",
    "\n",
    "      plt.ylabel('')\n",
    "      plt.title('Category Distribution')\n",
    "      plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1l3_-Xt5Mngv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "collapsed": true,
    "id": "1l3_-Xt5Mngv",
    "outputId": "dc6ea134-c784-4df3-bc43-20c55fbbd598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mINFO | Starting the model...\u001B[0m\n",
      "\u001B[92mINFO | Initializing Repository Handler\u001B[0m\n",
      "\u001B[92mINFO | Loading Data from Excel...\u001B[0m\n",
      "\u001B[92mINFO | Loaded 3855 SQLi, 86 RFI, 2696 LFI, 3102 XSS and 932 RCE entries.\u001B[0m\n",
      "\u001B[92mINFO | Loaded 10558 normal and 10671 malicious entries.\u001B[0m\n",
      "\u001B[92mINFO | Randomizing dataset 5 times...\u001B[0m\n",
      "\u001B[92mINFO | Dataset successfully randomized!\u001B[0m\n",
      "\u001B[92mINFO | Repository successfully initialized!\u001B[0m\n",
      "\u001B[92mINFO | Unique UriQuery values: 21229\u001B[0m\n",
      "\u001B[92mINFO | label2id: {'Benign': 0, 'LFI': 1, 'XSS': 2, 'SQLi': 3, 'RCE': 4, 'RFI': 5}\u001B[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294a8e9f05f141da9a7059cba62c233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Starting the model...\")\n",
    "\n",
    "try:\n",
    "    data = repo_handler()\n",
    "    unique_values = data.full_data.unique()\n",
    "    log.info(f\"Unique UriQuery values: {len(unique_values)}\")\n",
    "    model = BERT(data.full_data_df)\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error in model execution: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N34JOAIoQeg6",
   "metadata": {
    "collapsed": true,
    "id": "N34JOAIoQeg6"
   },
   "outputs": [],
   "source": [
    "#data.full_data_df['category_num'] = pd.factorize(data.full_data_df.category)[0]\n",
    "print(data.full_data_df.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
