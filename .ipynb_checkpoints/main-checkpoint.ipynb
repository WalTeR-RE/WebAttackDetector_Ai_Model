{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "LOG_DIR = \"./Logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def remove_logger_handlers(logger_name):\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "class CoolLogger:\n",
    "    def __init__(self, name=\"CoolLogger\", log_file=\"app.log\", level=logging.DEBUG):\n",
    "        \"\"\"\n",
    "        Initializes the logger with console and file handlers.\n",
    "\n",
    "        :param name: Logger name\n",
    "        :param log_file: Log file path\n",
    "        :param level: Logging level (default: DEBUG)\n",
    "        \"\"\"\n",
    "        remove_logger_handlers(name)\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(level)\n",
    "\n",
    "        log_format = logging.Formatter(\n",
    "            \"%(asctime)s | %(levelname)s | %(message)s\", \"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(self._get_colored_formatter())\n",
    "\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(log_format)\n",
    "\n",
    "        self.logger.addHandler(console_handler)\n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "    def _get_colored_formatter(self):\n",
    "        \"\"\" Returns a colorized log formatter \"\"\"\n",
    "        class ColoredFormatter(logging.Formatter):\n",
    "            COLORS = {\n",
    "                \"DEBUG\": \"\\033[94m\",  # Blue\n",
    "                \"INFO\": \"\\033[92m\",  # Green\n",
    "                \"WARNING\": \"\\033[93m\",  # Yellow\n",
    "                \"ERROR\": \"\\033[91m\",  # Red\n",
    "                \"CRITICAL\": \"\\033[41m\",  # Background Red\n",
    "                \"RESET\": \"\\033[0m\",\n",
    "            }\n",
    "\n",
    "            def format(self, record):\n",
    "                log_color = self.COLORS.get(record.levelname, self.COLORS[\"RESET\"])\n",
    "                reset = self.COLORS[\"RESET\"]\n",
    "                return f\"{log_color}{record.levelname} | {record.getMessage()}{reset}\"\n",
    "\n",
    "        return ColoredFormatter(\"%(levelname)s | %(message)s\")\n",
    "\n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def warning(self, message):\n",
    "        self.logger.warning(message)\n",
    "\n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "\n",
    "    def critical(self, message):\n",
    "        self.logger.critical(message)\n",
    "\n",
    "\n",
    "logging.root.handlers.clear()\n",
    "remove_logger_handlers(\"RepoHandlerLogger\")\n",
    "remove_logger_handlers(\"ModelLogger\")\n",
    "\n",
    "# Initialize loggers\n",
    "log = CoolLogger(name=\"RepoHandlerLogger\", log_file=f\"{LOG_DIR}/repo_handler.log\")\n",
    "model_log = CoolLogger(name=\"ModelLogger\", log_file=f\"{LOG_DIR}/xg_trainer.log\")"
   ],
   "metadata": {
    "id": "FzOuiQoTObOB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739893809115,
     "user_tz": -120,
     "elapsed": 289,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     }
    },
    "ExecuteTime": {
     "start_time": "2025-02-18T17:59:06.809922Z",
     "end_time": "2025-02-18T17:59:06.857338Z"
    }
   },
   "id": "FzOuiQoTObOB",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "class repo_handler:\n",
    "\n",
    "    def __init__(self):\n",
    "        log.info(\"Initializing Repository Handler\")\n",
    "        self.malicious_data = []\n",
    "        self.malicious_data_df = []\n",
    "        self.LFI_df = []\n",
    "        self.SQLi_df = []\n",
    "        self.XSS_df = []\n",
    "        self.RFI_df = []\n",
    "        self.RCE_df = []\n",
    "        self.normal_data = []\n",
    "        self.normal_data_df = []\n",
    "        self.excel_file = \"./Repository/Dataset.xlsx\"\n",
    "\n",
    "        try:\n",
    "            self.load_data()\n",
    "            self.full_data = pd.concat([self.normal_data, self.malicious_data], ignore_index=True)\n",
    "            self.malicious_data_df = pd.concat([self.RCE_df,self.LFI_df,self.RFI_df,self.SQLi_df,self.XSS_df])\n",
    "            self.normal_data_df = pd.DataFrame(self.normal_data, columns=[\"UriQuery\"])\n",
    "            self.normal_data_df[\"category\"] = \"Benign\"\n",
    "            self.malicious_data_df[\"isVulnerable\"] = 1 # 1 for malicious\n",
    "            self.normal_data_df[\"isVulnerable\"] = 0 # 0 for normal\n",
    "            self.full_data_df = pd.concat([self.malicious_data_df,self.normal_data_df],ignore_index=True)\n",
    "            self.randomize_n_time(5)\n",
    "            log.info(\"Repository successfully initialized!\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during initialization: {e}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\" Loads and processes data from the Excel file. \"\"\"\n",
    "        log.info(\"Loading Data from Excel...\")\n",
    "\n",
    "        try:\n",
    "            excel_data = pd.read_excel(self.excel_file, sheet_name=None)\n",
    "            data = {\n",
    "                sheet_name: d[\"UriQuery\"].drop_duplicates().dropna()\n",
    "                for sheet_name, d in excel_data.items() if \"UriQuery\" in d.columns\n",
    "            }\n",
    "\n",
    "            if not data:\n",
    "                log.warning(\"No data found in the Excel file!\")\n",
    "\n",
    "            self.normal_data = data[\"Benign\"]\n",
    "            self.RCE_df = pd.DataFrame(data[\"RCE\"], columns=[\"UriQuery\"])\n",
    "            self.RCE_df[\"category\"] = \"RCE\"\n",
    "\n",
    "            self.LFI_df = pd.DataFrame(data[\"LFI\"], columns=[\"UriQuery\"])\n",
    "            self.LFI_df[\"category\"] = \"LFI\"\n",
    "\n",
    "            self.SQLi_df = pd.DataFrame(data[\"SQLi\"], columns=[\"UriQuery\"])\n",
    "            self.SQLi_df[\"category\"] = \"SQLi\"\n",
    "\n",
    "            self.XSS_df = pd.DataFrame(data[\"XSS\"], columns=[\"UriQuery\"])\n",
    "            self.XSS_df[\"category\"] = \"XSS\"\n",
    "\n",
    "            self.RFI_df = pd.DataFrame(data[\"RFI\"], columns=[\"UriQuery\"])\n",
    "            self.RFI_df[\"category\"] = \"RFI\"\n",
    "\n",
    "\n",
    "            self.malicious_data = pd.concat([data[\"RCE\"],data[\"LFI\"],data[\"SQLi\"],data[\"XSS\"],data[\"RFI\"]], ignore_index=True)\n",
    "            log.info(f\"Loaded {len(data['SQLi'])} SQLi, {len(data['RFI'])} RFI, {len(data['LFI'])} LFI, {len(data['XSS'])} XSS and {len(data['RCE'])} RCE entries.\")\n",
    "            log.info(f\"Loaded {len(self.normal_data)} normal and {len(self.malicious_data)} malicious entries.\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error loading data: {e}\")\n",
    "\n",
    "    def randomize_n_time(self, counter):\n",
    "        \"\"\" Randomizes the dataset multiple times. \"\"\"\n",
    "        log.info(f\"Randomizing dataset {counter} times...\")\n",
    "\n",
    "        try:\n",
    "            for _ in range(counter):\n",
    "                self.full_data_df = self.full_data_df.sample(frac=1).reset_index(drop=True)\n",
    "            log.info(\"Dataset successfully randomized!\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during randomization: {e}\")\n"
   ],
   "metadata": {
    "id": "X69rUbz6OYNX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739893812815,
     "user_tz": -120,
     "elapsed": 1414,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     }
    },
    "ExecuteTime": {
     "start_time": "2025-02-18T17:59:06.831321Z",
     "end_time": "2025-02-18T17:59:07.648394Z"
    }
   },
   "id": "X69rUbz6OYNX",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVh50_RPg3ug",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739893822161,
     "user_tz": -120,
     "elapsed": 7136,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     }
    },
    "outputId": "344cf909-cade-43dd-9c23-7ea1e7500351",
    "ExecuteTime": {
     "start_time": "2025-02-18T17:59:07.652796Z",
     "end_time": "2025-02-18T17:59:10.810978Z"
    }
   },
   "id": "lVh50_RPg3ug",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class WebAttackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "          item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ],
   "metadata": {
    "id": "WHZBoQepk9Es",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739893822161,
     "user_tz": -120,
     "elapsed": 38,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     }
    },
    "ExecuteTime": {
     "start_time": "2025-02-18T17:59:10.818555Z",
     "end_time": "2025-02-18T17:59:10.893747Z"
    }
   },
   "id": "WHZBoQepk9Es",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
    "#from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self,data_frame=None):\n",
    "        self.data = data_frame\n",
    "        self.model = None\n",
    "        self.id2label,self.label2id = self.labelization()\n",
    "        #model_log.info(f\"id2label: {self.id2label}\")\n",
    "        model_log.info(f\"label2id: {self.label2id}\")\n",
    "        self.represent()\n",
    "        self.train_model()\n",
    "        #self.create_model()\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "      X_train, X_val, y_train, y_val = train_test_split(list(self.data[\"UriQuery\"]),list(self.data[\"categories_label\"]), test_size=0.2,stratify=self.data[\"categories_label\"])\n",
    "      tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "      X_train_tokenized = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "      X_val_tokineized = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "      train_dataset = WebAttackDataset(X_train_tokenized, y_train)\n",
    "      val_dataset = WebAttackDataset(X_val_tokineized, y_val)\n",
    "      training_args = TrainingArguments(\n",
    "          output_dir=\"./Results\",\n",
    "          num_train_epochs=3,\n",
    "          per_device_train_batch_size=16\n",
    "      )\n",
    "\n",
    "      self.load_model()\n",
    "\n",
    "      trainer = Trainer(\n",
    "          model=self.model,\n",
    "          args=training_args,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=val_dataset,\n",
    "          compute_metrics=self.compute_metric\n",
    "\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "      trainer.evaluate()\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "      #print(self.model)\n",
    "\n",
    "      training_args = TrainingArguments(\n",
    "      output_dir=\"./Results\",  # Directory to store model and logs\n",
    "      num_train_epochs=3,      # Number of epochs\n",
    "      per_device_train_batch_size=16,  # Batch size for training\n",
    "      per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "      eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "      save_strategy=\"epoch\",          # Save model at the end of each epoch\n",
    "      logging_dir=\"./Logs\",           # Directory for logs\n",
    "      logging_steps=200,              # Log after every 200 steps\n",
    "      load_best_model_at_end=True,    # Load the best model based on evaluation\n",
    "      metric_for_best_model=\"accuracy\",  # Metric to track for the best model\n",
    "      # Optional: To improve checkpoint saving\n",
    "      save_total_limit=3,             # Keep only the last 3 saved checkpoints\n",
    "      weight_decay=0.01,              # Apply weight decay to reduce overfitting\n",
    "      warmup_steps=500,               # Warm-up for the first 500 steps to stabilize training\n",
    "      fp16=True,                      # Use mixed precision (if supported by your hardware for faster training)\n",
    "      seed=42,                        # Set random seed for reproducibility\n",
    "      )\n",
    "\n",
    "      trainer = Trainer(\n",
    "      model=self.model,                  # Pretrained BERT model\n",
    "      args=training_args,           # Training arguments\n",
    "      train_dataset=train_dataset,  # Training dataset\n",
    "      eval_dataset=val_dataset,     # Validation dataset\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "\n",
    "      results = trainer.evaluate()\n",
    "      print(results)\n",
    "      \"\"\"\n",
    "\n",
    "    def compute_metric(self,p):\n",
    "      print(type(p))\n",
    "      pred, labels = p\n",
    "      pred = np.argmax(pred, axis=1)\n",
    "\n",
    "      accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "      recall = recall_score(y_true=labels, y_pred=pred)\n",
    "      precision = precision_score(y_true=labels, y_pred=pred)\n",
    "      f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "      return {\"Accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    def labelization(self):\n",
    "      labels = self.data['category'].unique().tolist()\n",
    "      labels = [s.strip() for s in labels]\n",
    "      return {id: label for id,label in enumerate(labels)},{label: id for id,label in enumerate(labels)}\n",
    "\n",
    "    def represent(self):\n",
    "      self.data[\"categories_label\"] = pd.factorize(self.data.category)[0]\n",
    "\n",
    "    def load_model(self):\n",
    "      self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(self.label2id))\n",
    "\n",
    "\n",
    "    def viewGraph(self):\n",
    "      self.data['category'].value_counts().plot(\n",
    "      kind='pie',\n",
    "      figsize=(8, 8),\n",
    "      autopct=lambda p: f'{p:.2f}%'\n",
    "      )\n",
    "\n",
    "      plt.ylabel('')\n",
    "      plt.title('Category Distribution')\n",
    "      plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "in8Vy9nqOQOw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739893844443,
     "user_tz": -120,
     "elapsed": 22320,
     "user": {
      "displayName": "Osama Aly (W4lT3R)",
      "userId": "14351702280299970694"
     }
    },
    "ExecuteTime": {
     "start_time": "2025-02-18T17:59:10.836813Z",
     "end_time": "2025-02-18T17:59:15.941128Z"
    }
   },
   "id": "in8Vy9nqOQOw",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "log.info(\"Starting the model...\")\n",
    "\n",
    "try:\n",
    "    data = repo_handler()\n",
    "    unique_values = data.full_data.unique()\n",
    "    log.info(f\"Unique UriQuery values: {len(unique_values)}\")\n",
    "    model = BERT(data.full_data_df)\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error in model execution: {e}\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "collapsed": true,
    "id": "1l3_-Xt5Mngv",
    "outputId": "dc6ea134-c784-4df3-bc43-20c55fbbd598"
   },
   "id": "1l3_-Xt5Mngv",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mINFO | Starting the model...\u001B[0m\n",
      "\u001B[92mINFO | Initializing Repository Handler\u001B[0m\n",
      "\u001B[92mINFO | Loading Data from Excel...\u001B[0m\n",
      "\u001B[92mINFO | Loaded 3855 SQLi, 86 RFI, 2696 LFI, 3102 XSS and 932 RCE entries.\u001B[0m\n",
      "\u001B[92mINFO | Loaded 10558 normal and 10671 malicious entries.\u001B[0m\n",
      "\u001B[92mINFO | Randomizing dataset 5 times...\u001B[0m\n",
      "\u001B[92mINFO | Dataset successfully randomized!\u001B[0m\n",
      "\u001B[92mINFO | Repository successfully initialized!\u001B[0m\n",
      "\u001B[92mINFO | Unique UriQuery values: 21229\u001B[0m\n",
      "\u001B[92mINFO | label2id: {'Benign': 0, 'LFI': 1, 'XSS': 2, 'SQLi': 3, 'RCE': 4, 'RFI': 5}\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "294a8e9f05f141da9a7059cba62c233b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#data.full_data_df['category_num'] = pd.factorize(data.full_data_df.category)[0]\n",
    "print(data.full_data_df.head())"
   ],
   "metadata": {
    "id": "N34JOAIoQeg6",
    "collapsed": true
   },
   "id": "N34JOAIoQeg6",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
